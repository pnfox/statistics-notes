% ------------------------------------------------------------------------------
% Chapter 2
% ------------------------------------------------------------------------------
\chapter{Discrete Probability Distributions}
\label{chapter2}

\par
In mathematics a variable is a symbol which represents a quantity, number, function, graph, or any mathematical object.
In computer science a variable is also a placeholder for quantities or (in higher level languages) objects. When a variables' values
are bound by randomness, we call these random variables. The random variable $X$ might represent the outcome of flipping a coin, while
the random variable $Y$ may be the price of milk next year.\newline \par

More formally, a \textbf{random variable} is a real-valued function whose domain is a sample space.\newline \par

In the case of our example variable $X$ the sample space is either heads or tails. For our variable $Y$ the sample space is any real number
(i.e. any number between $-\infty$ and $\infty$). When the sample space only constitues a finite set of values, we say that the random variable
is discrete. Otherwise it is called a continious random variable, even if the price of milk can only be between $0$ and $10$, the fact it lies randomly on
a continious interval means it is a continious random variable. To be more specific: \newline \par

A random variable $X$ is said to be \textbf{discrete} if it can take on only a finite number - or a countably infinite number - of possible values of x
The probability function of $X$, denoted $p(x)$, assigns probability to each value of x of $X$ so that the following conditions are satisfied.

\begin{enumerate}
	\item $P(X=x) = p(x) \geq 0$
	\item $\sum_{x}P(X=x)=1$ where the sum is over all possible values of x
\end{enumerate}\par

It is often useful in some cases to study random variables by looking at cumulative probabilities; that is, we
look at the probability a random variable $X$ takes a value less than or equal to some value $x$, i.e. $P(X\leq x)$.
This is described by the \textbf{cumulative distribution function}, denoted by $F(x)$.\newline \par

Hence the cumulative distribution function $F(x)$ for a randon variable $X$ is
\begin{align*}
	F(b)&=P(X\leq x) \\
	\textrm{if }&X\textrm{ is discrete then,} \\
	F(b)&=\sum_{x=-\infty}^{b}p(x)
\end{align*}
where $p(x)$ is the probability function.\newline \par

Even though the value of $X$ can be anything from a finite set of values, the cumulative distribution function
is actually a discontinuous step function. This is because $F(b)$ is defined for
all real numbers but will only increase where $X$ has probabilities associated with specific values.\newline

\section{Expected Values and Variation}

Even if we have a full picture of how likely different outcomes may be, it can still be difficult to see
what the usual expected value, as this may not always be the most likely outcome. In probability, the average
value of a large number of independent realizations of $X$ is called the \textbf{expected value}, often denoted
with $E(X)$ or $E[X]$. We define this formally as,

\begin{equation*}
	E(X) = \sum_{x}xp(x)
\end{equation*}

As you can see, the expected value is the sum of values $X$ can take, $x$, each multiplied by the probability associated with that value, $p(x)$.
The expected value therefore is a weighted average, as values with associated higher probabilities are more likely
to shift $E(X)$ towards the value of $x$ (when $p(x)$ is close to $1$).\newline \par

If the values $X$ can take are governed by a real-valued function $g(x)$ then the expected value is as follows,

\begin{equation*}
	E(g(X)) = \sum_{x}g(x)p(x)
\end{equation*}

In order numerically represent how spread out values are around the expected value, i.e. the variation in our data, we calculate \textbf{variance},
which is given by (where $\mu = E(X)$)

\begin{align*}
	V(X)&=E[(X-\mu)^2]\\
	& \textrm{which for a discrete random variable is} \\
	&=\sum_{x}(x-\mu)^{2}p(x)
\end{align*}

Variance is sometimes expressed with the notation $\sigma ^2$, because the variance is equal to the \textbf{standard deviation} squared. The standard deviation
is a measure of variation that maintains the original units of measure, as opposed to the variance which is in squared units.\newline \par

\textbf{Example:} Assume we have the following data:

\begin{center}
\begin{tabular}{|c c c|} 
 \hline
 Age Interval & Age Midpoint & \% population \\ [0.5ex] 
 \hline\hline
 $<$ 5 & 3 & 6.9 \\ 
 \hline
 5-9 & 8 & 7.3 \\
 \hline
 10-19 & 15 & 14.4 \\
 \hline
 20-29 & 25 & 13.3 \\
 \hline
 30-39 & 35 & 15.5 \\
 \hline
 40-49 & 45 & 15.3 \\
 \hline
 50-59 & 55 & 10.8 \\
 \hline
 $>$ 60 & 80 & 16.5 \\ [1ex]
 \hline
\end{tabular}
\end{center}

Even though we are given the age as a continuous interval, we calculate the midpoint of these to give us
a discrete variable of age. Interpreting the percentages as probabilities, we calculate the mean age as follows.

\begin{align*}
	\mu = E(X) & = \sum_{x}xp(x) \\
	& = 3\cdot 0.069 + 8\cdot 0.073 + 15\cdot 0.144 + ... + 80\cdot 0.165 \\
	& = 37.7
\end{align*}

We can calculate how spread the values are around the mean, with the variance.

\begin{align*}
	\sigma ^2 & = \sum_{x}(x-\mu)^{2}p(x) \\
	& = \sqrt{(3-37.7)^2(0.069) + (8-37.7)^2(0.073) + ... + (80-37.7)^2(0.165)}
	& = 579.9
\end{align*}

As you can see a variance of 579.9 is a hard number to interpret in terms of age squared. This is why it is often more useful
to calculate the standard deviation as it is in the original units (age). Which is $\sigma = \sqrt{579.9} = 24.08$. In summary, we can
see that the expected value (or mean) age of the population is $37.7$ years old, while on average a person deviates by $\pm 24.08$ years around this mean.

\section{Chebyshev's inequality}
The mean and variance give us an idea for the center and spread of a distribution. We can infact calculate intervals about the mean. Chebyshev's inequality
provides us a way to do that without needing to know the type of distribution (normal, bernoulli, exponential,...) a variable takes, which is incredibly useful
as many statistical techniques often require a variable be of a certain distribution type in order to work. Chebyshev's inequality however can be applied to
any probability distribution.\newline

Let $X$ be a random variable with mean $\mu$ and variance $\sigma ^2$. Then for any positive $k$,

\begin{equation*}
	P(\left \| X - \mu\right \| < k\sigma) \geq 1 - \frac{1}{k^2}
\end{equation*}

Which is equivalent to saying,

\begin{equation*}
	P(\mu - k\sigma < X < \mu + k\sigma) \geq 1 - \frac{1}{k^2}
\end{equation*}

Let's calculate the 90\%confidence interval from our previous example. We set $(1-1/k^2)$ equal to 0.9 and solve for $k$.

\begin{align*}
	1-\frac{1}{k^2} & = 0.9 \\
	\frac{1}{k^2} & = 0.1 \\ 
	k^2 & = 10 \\
	k & = \sqrt{10} = 3.16
\end{align*}

Then the confidence interval is

\begin{align*}
	(\mu - 3.16\cdot \sigma,\; \mu + 3.16\cdot \sigma) & = (37.7 - 3.16\cdot 24.08,\; 37.7 + 3.16\cdot 24.08) \\
	& = (-10.46, 85.86)
\end{align*}

Of course negative ages don't make sense but 90\% of our the population has an age within this interval.
It cannot be assumed that half the values are on the left of this interval and the other half are on the right side of the interval.
